
#-------------------------------------------------------------------------------
#
# BigDataScript configuration file
#
#																Pablo Cingolani
#-------------------------------------------------------------------------------

#---
# Mesos parameters
#---

mesos.master = 127.0.0.1:5050

#---
# SSH cluster nodes stored here
# 
# Format: user@host[:port]
#---

# Ssh cluster: Localhost (testing)
ssh.nodes = user@localhost

# Some nodes for 'ssh cluster'
#ssh.nodes = user@lab1-1company.com, user@lab1-2company.com, user@lab1-3company.com, user@lab1-4company.com, user@lab1-5company.com, user@lab1-6company.com, user@lab1-7company.com, user@lab1-8company.com, user@lab1-9company.com, user@lab1-10company.com, user@lab1-11company.com, user@lab1-12company.com, user@lab1-13company.com, user@lab1-14company.com, user@lab1-15company.com, user@lab1-16company.com \
# 			, user@lab2-1company.com, user@lab2-2company.com, user@lab2-3company.com, user@lab2-4company.com, user@lab2-5company.com, user@lab2-6company.com, user@lab2-7company.com, user@lab2-8company.com, user@lab2-9company.com, user@lab2-10company.com, user@lab2-11company.com, user@lab2-12company.com, user@lab2-13company.com, user@lab2-14company.com, user@lab2-15company.com, user@lab2-16company.com, user@lab2-17company.com, user@lab2-18company.com, user@lab2-19company.com, user@lab2-20company.com, user@lab2-21company.com, user@lab2-22company.com, user@lab2-23company.com, user@lab2-24company.com, user@lab2-25company.com, user@lab2-26company.com, user@lab2-27company.com, user@lab2-28company.com, user@lab2-29company.com, user@lab2-30company.com, user@lab2-31company.com, user@lab2-32company.com, user@lab2-33company.com, user@lab2-34company.com, user@lab2-35company.com, user@lab2-36company.com, user@lab2-37company.com, user@lab2-38company.com, user@lab2-39company.com, user@lab2-40company.com, user@lab2-41company.com, user@lab2-42company.com, user@lab2-43company.com, user@lab2-44company.com, user@lab2-45company.com, user@lab2-46company.com, user@lab2-47company.com, user@lab2-48company.com, user@lab2-49company.com \
# 			, user@lab3-1company.com, user@lab3-2company.com, user@lab3-3company.com, user@lab3-4company.com, user@lab3-5company.com, user@lab3-6company.com, user@lab3-7company.com, user@lab3-8company.com, user@lab3-9company.com, user@lab3-10company.com, user@lab3-11company.com, user@lab3-12company.com, user@lab3-13company.com, user@lab3-14company.com, user@lab3-15company.com, user@lab3-16company.com, user@lab3-17company.com, user@lab3-18company.com, user@lab3-19company.com, user@lab3-20company.com, user@lab3-21company.com, user@lab3-22company.com, user@lab3-23company.com, user@lab3-24company.com, user@lab3-25company.com, user@lab3-26company.com, user@lab3-27company.com, user@lab3-28company.com
#

# AWS server farm using ssh (nodes started using StarCluster)
#ssh.nodes = sgeadmin@node001, sgeadmin@node002, sgeadmin@node003, sgeadmin@node004, sgeadmin@node005, sgeadmin@node006

#---
# Default parameters
#---

# Default memory (-1 = unrestricted)
#mem = -1

# Default execution node (none)
#node = ""

# Regex used to extract PID from cluster command (e.g. qsub). 
# Default, use the whole line
# Note: Some clusters add the domain name to the ID and 
#       then never use it again, some other clusters add 
#       a message (e.g. 'You job ...')
#pidRegex = ""
#pidRegex = "(.+).domain.com"
#pidRegex = "Your job (\\S+)"

# Add default queue name (if any)
#queue = ""

# Default number of retries
#retry = 0

# Default system is 'local' (run on local computer)
#system = "local"

# Task timeout in seconds (default is one day)
#timeout = 86400

# Task's wall-timeout in seconds (default is one day)
#walltimeout = 86400

#---
# SGE parameters
#---

# Parallel environment in SGE (e.g. 'qsub -pe mpi 4')
# 
# Note on SGE's parallel environment ('-pe'):
#   Parallel environment defines how 'slots' (number of cpus requested) 
#   are allocated. StarCluster by default sets up a parallel environment, called “orte”, 
#   that has been configured for OpenMPI integration within SGE and has a number of slots 
#   equal to the total number of processors in the cluster.
#   See details 'qconf -sp orte':
#         pe_name            orte
#         slots              16
#         user_lists         NONE
#         xuser_lists        NONE
#         start_proc_args    /bin/true
#         stop_proc_args     /bin/true
#         allocation_rule    $round_robin
#         control_slaves     TRUE
#         job_is_first_task  FALSE
#         urgency_slots      min
#         accounting_summary FALSE
#         
#   Notice the allocation_rule = $round_robin.  This defines how to assign slots to a job. By 
#   default StarCluster configures round_robin allocation. This means that if a job requests 8 
#   slots for example, it will go to the first machine, grab a single slot if available, move to 
#   the next machine and grab a single slot if available, and so on wrapping around the cluster 
#   again if necessary to allocate 8 slots to the job.
#   You can also configure the parallel environment to try and localize slots as much as 
#   possible using the "fill_up" allocation rule and job_is_first_task of TRUE.
#   To configure: qconf -mp orte
#
#   References: 
#   	http://star.mit.edu/cluster/docs/0.93.3/guides/sge.html#openmpi-parallel-environment
#   	https://blogs.oracle.com/templedf/entry/configuring_a_new_parallel_environment
#
sge.pe = shm

# Parameter for requesting amount of memory in qsub (e.g. 'qsub -l mem 4G')
sge.mem = h_vmem

# Parameter for timeout in qsub (e.g. 'qsub -l h_rt 24:00:00')
sge.timeout = h_rt

clusterRunAdditionalArgs = -V -l h_rt=08:00:00 -l h_vmem=12G
