#!/usr/bin/env bds

include "general.bds"

url_base 	:= "" 		help URL base for output directory.

padding_left	:= 20


void init_report() {

	if ( conf_file_exists() ) {
		url_base	= get_conf_val( url_base, 		["url_base"] )
	}
	
	if ( v ) { // verbose
		print("\n")
		print( "URL root for output directory\t: $url_base\n" )
	}
}

// bwa logs (*.flagstat.qc): 1 table
string _parse_flagstat_to_html( string id, string[] names, string[] logs, string[] header ) {

	if ( logs.size() == 0 ) return ""

	html := "<div id='flagstat_$id'><b>Flagstat QC ($id)</b><div style='padding-left: "+padding_left+"px'>"

	html += _html_table_header( ["&nbsp","Reads (QC-passed)","Reads (QC-failed)","Dupes (QC-passed)","Dupes (QC-failed)","Mapped Reads","% Mapped"] )

	// parse dup.qc
	for ( int i=0; i<logs.size(); i++ ) {

		// quite ugly parsing.... sorry
		log := logs[i]
		name := names[i]
		lines := log.readLines()

		tmp1 := lines[0].split(" in total ")
		line1 := tmp1[0]
			tmp1 = line1.split( " \\+ " )
			read1 := tmp1[0]
			read2 := tmp1[1]

		tmp2 := lines[1].split(" duplicates")
		line2 := tmp2[0]
			tmp2 = line2.split( " \\+ " )
			dupe1 := tmp2[0]
			dupe2 := tmp2[1]

		tmp3 := lines[2].split(" mapped \\(")
		line3_1 := tmp3[0]
			tmp3_1 := line3_1.split( " \\+ " )
			mapped := tmp3_1[0]
		line3_2 := tmp3[1]
			tmp3_2 := line3_2.split( "\\%\\:" )
			percent := tmp3_2[0]

		html += "<tr><td>"+_html_link( log, name )+"</td><td>$read1</td><td>$read2</td><td>$dupe1</td><td>$dupe2</td><td>$mapped</td><td>$percent</td></tr>"
	}

	html += "</table>"
	html += "</div></div><br>"

	return html
}

// MarkDuplicate (post-align) logs (*.dup.qc): 1 table
string _parse_dup_to_html( string id, string[] names, string[] logs, string[] headers ) {

	if ( logs.size() == 0 ) return ""

	html := "<div id='dup_$id'><b>Dup QC ($id)</b><div style='padding-left: "+padding_left+"px'>"

	html += _html_table_header( ["&nbsp","Unpaired Reads", "Paired Reads", "Unmapped Reads", "Unpaired Dupes", "Paired Dupes", "Paired Opt. Dupes", "% Dupes"] )

	// parse dup.qc
	for ( int i=0; i<logs.size(); i++ ) {

		log := logs[i]
		name := names[i]
		line := get_stdout( "cat $log | grep 'Unknown Library' | awk '{ print $3,$4,$5,$6,$7,$8,$9; }'" )	

		html += "<tr><td>"+_html_link( log, name )+"</td><td>" + line.replace(" ","</td><td>") + "</td></tr>"
	}

	html += "</table>"
	html += "</div></div><br>"

	return html
}

// Library complexcity (post-align) logs (*.pbc.qc) : 1 table
string _parse_pbc_to_html( string id, string[] names, string[] logs, string[] header ) {

	if ( logs.size() == 0 ) return ""

	html := "<div id='pbc_$id'><b>PBC QC ($id)</b><div style='padding-left: "+padding_left+"px'>"

	html += _html_table_header( ["&nbsp","Total Read Pairs","Distinct Read Pairs","One Read Pair","Two Read Pairs","NRF = Distinct/Total","PBC1 = OnePair/Distinct","PBC2 = OnePair/TwoPair"] )

	// parse pbc.qc
	for ( int i=0; i<logs.size(); i++ ) {

		log := logs[i]
		name := names[i]
		line := log.read()

		html += "<tr><td>"+_html_link( log, name )+"</td><td>" + line.replace("\t","</td><td>") + "</td></tr>"
	}

	html += "</table>"

	// help
	html += "<p>" + "NRF (non redundant fraction) <br>" + \
			"PBC1 (PCR Bottleneck coefficient 1) <br>" + \
			"PBC2 (PCR Bottleneck coefficient 2) <br>" + \
			"PBC1 is the primary measure. Provisionally <br>" + \
			"<ul>" + \
			"<li>0-0.5 is severe bottlenecking</li>" + \
			"<li>0.5-0.8 is moderate bottlenecking </li>" + \
			"<li>0.8-0.9 is mild bottlenecking </li>" + \
			"<li>0.9-1.0 is no bottlenecking </li>" + \
			"</ul>" + \
			"</p>"

	html += "</div></div><br>"

	return html
}

// Cross correlation analysis log (*.cc.qc): 1 table + img (png converted from pdf)
string _parse_xcor_to_html( string id, string[] names, string[] logs, string[] pdfs, string[] header ) {

	if ( logs.size() == 0 ) return ""

	html := "<div id='xcor_$id'><b>Cross-correlation QC ($id)</b><div style='padding-left: "+padding_left+"px'>"

	html += _html_table_header( ["&nbsp","numReads","estFragLen","corr_estFragLen","PhantomPeak","corr_phantomPeak","argmin_corr","min_corr","phantomPeakCoef","relPhantomPeakCoef","QualityTag"] )

	// parse cc.qc
	for ( int i=0; i<logs.size(); i++ ) {

		log := logs[i]
		name := names[i]

		items := log.read().split("\t")
		items.removeIdx(0) // remove file name
		
		html += "<tr><td>"+_html_link( log, name )+"</td><td>" + items.join("\t").replace("\t","</td><td>") + "</td></tr>"
	}

	html += "</table>"

	// help
	html += "<p>" + "Normalized strand cross-correlation coefficient (NSC) = col9 in outFile <br>" + \
			"Relative strand cross-correlation coefficient (RSC) = col10 in outFile <br>" + \
			"Estimated fragment length = col3 in outFile, take the top value <br>" + \
			"Important columns highlighted, but all/whole file can be stored for display <br>" + \
			 "</p><br>"

	// images
	for ( int i=0; i<pdfs.size(); i++ ) {

		png := _pdf_to_png( pdfs[i] )

		html += _html_img( png, 500, header[i] ) + "&nbsp"
	}	

	html += "</div></div><br>"

	return html
}

// IDR logs : 1 table + 4 imgs
string _parse_idr_to_html( string id, string log ) {

	if ( log == "" ) return ""

	html := "<div id='$id'><b>IDR QC ($id)</b><div style='padding-left: "+padding_left+"px'>"

	html += _html_link( log, get_basename( log ) )

	html += _html_table_header( ["Nt","N1","N2","Np","conservative_set","optimal_set","rescue_ratio","self_consistency_ratio","reproducibility"] )

	lines := log.readLines()

	html += "<tr><td>" + lines[1].replace("\t","</td><td>") + "</td></tr>" // read 2nd line of idr qc file

	html += "</table>"
	html += "</div></div><br>"

	return html
}

// WashU Epigenome browser datahub (json format)
string _html_epg_browser_viz( string[] files, string[] types, string[] names ) {

	if ( files.size() == 0 ) return ""

	html := "<div id='viz'><b>Visualization</b><div style='padding-left: "+padding_left+"px'>"

	json := "[ " // json array starts here

	for ( int i=0; i<files.size(); i++ ) {

		file := files[i]
		type := types[i]
		trackname := names[i]

		// add url_base
		url 	:= get_url( file )

		if ( type == "bigwig") {
			
			json 	+= 	'{ "type": "bigwig", "name": "'+trackname+'", "url": "'+url+'", "mode": 1,' + \
					'  "qtc" : { "height": 30, "summeth":2, "smooth":3} },'
		}
		else if ( type == "hammock") { // peak must be converted to hammock type (browser specific format)	

			json 	+= 	' { "type": "hammock", "name": "'+trackname+'", "url": "'+url+'", "mode": "barplot", "showscoreidx":1,' + \
					'   strokecolor:"#ff6600", scorescalelst:[{"type":1,"min":0,"max":40},{"type":0},{"type":0},{"type":0}],' + \
					'   scorenamelst:["signal value", "P value (-log10)","Q value (-log10)"],' + \
					' "qtc": { "height" : 30, "summeth":2, "fontsize":"0pt","fontfamily":"sans-serif"} },'
		}
	}

	// add ref genome track
	json += '{ "type":"native_track", "list":[ { "name":"refGene", "mode":"full", } ] },'

	json += " ]" // json array ends here

	json_file := "$out_dir/tracks.json"
	json_file.write( json )

	json_url := get_url( json_file )
	viz_url	 := "http://epigenomegateway.wustl.edu/browser/?genome=" + species + "&tknamewidth=150&datahub=" + json_url

	html += "<a href=$viz_url target='_blank'>Visualize</a>&nbsp&nbsp"
	html += "<a href=$json_url target='_blank'>JSON (datahub)</a>"
	html += "</div></div><br>\n\n"

	return html
}

string _html_table_header( string[] head ) {

	html := "<table border='1'><tr>"

	for ( int i=0; i<head.size(); i++ ) {
		html += "<th>"+head[i]+"</th>"
	}
	
	return html + "</tr>"
}

string _html_link( string path, string name ) {

	return "<a href='" + get_rel_path( path ) + "' target='_blank'>$name</a><br>"
}

string _html_link_url( string path ) {

	return "<a href='" + get_url( path ) + "' target='_blank'>" + get_rel_path( path ) + "</a><br>"
}

string _html_img( string path, int width, string cap ) {

	return "<figure style='display: inline-block;'><img src='" + get_rel_path( path ) + "' width='$width'><figcaption style='text-align: center;'><b>$cap</b></figcaption></figure>"
}

string _pdf_to_png( string pdf ) { 

	png 	:= remove_ext( pdf, "pdf" ) + ".png"

	// Requirements: poppler-utils (sudo apt-get install poppler-utils)	
	sys pdftoppm -png $pdf > $png

	return png
}

string _peak_to_hammock( string peak ) {

	prefix 	:= remove_ext( peak, ["gz"] )

	hammock := "$prefix.hammock"
	tmp 	:= "$prefix.tmp"

	// choose correct converter .py : narrowpeak (regionpeak), broadpeak, gappedpeak to hammock
	// they are under git_repo_root/utils/
	
	conv := programPath.dirName()+"/utils/"

	if ( peak.toLower().indexOf( "regionpeak" ) > -1 ) 	conv += "narrowpeak.py"
	else if ( peak.toLower().indexOf( "narrowpeak" ) > -1 ) conv += "narrowpeak.py"
	else if ( peak.toLower().indexOf( "broadpeak" ) > -1 ) 	conv += "broadpeak.py"
	else if ( peak.toLower().indexOf( "gappedpeak" ) > -1 ) conv += "gappedpeak.py"

	in 	:= peak
	out 	:= hammock+".gz"

	if ( out<-in ) { // needs bgzip and tabix

		sys zcat $peak | sed '/^\(chr\)/!d' | sort -k1,1V -k2,2n > $tmp
		sys python $conv $tmp $hammock
	}

	return out	
}

string get_url( string path ) {

	rel_path := get_rel_path( path )

	if ( rel_path == "" ) 	return ""
	else 			return url_base + "/" + get_rel_path( path )
}